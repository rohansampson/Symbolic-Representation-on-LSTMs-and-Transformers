{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Code originally by https://github.com/maxjcohen\n",
    "# My contribution is refine, fix errors and to attempt symbolic representation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import sys\n",
    "from ABBA import ABBA as ABBA\n",
    "import warnings\n",
    "\n",
    "sys.path.append('C:/Users/rohan/Dissertation/transformer-master/transformer-master')\n",
    "sys.path.append('C:/Users/rohan/Dissertation/transformer-master/transformer-master/src')\n",
    "sys.path.append('C:/Users/rohan/Dissertation/transformer-master/transformer-master/dataset')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Access and parse dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "labels_path: Optional[str] = \"labels.json\"\n",
    "normalize: Optional[str] = \"max\"\n",
    "_normalize = normalize\n",
    "\n",
    "# _load_npz(dataset_path, labels_path)\n",
    "dataset_path = 'dataset/dataset.npz'\n",
    "\n",
    "dataset = np.load(dataset_path)\n",
    "\n",
    "with open(labels_path, \"r\") as stream_json:\n",
    "    labels = json.load(stream_json)\n",
    "    \n",
    "R = dataset['R'].astype(np.float32)\n",
    "X = dataset['X'].astype(np.float32)\n",
    "Z = dataset['Z'].astype(np.float32)\n",
    "\n",
    "m = Z.shape[0]  # Number of training example\n",
    "K = Z.shape[-1]  # Time serie length\n",
    "\n",
    "Z = Z.transpose((0, 2, 1))\n",
    "X = X.transpose((0, 2, 1))\n",
    "\n",
    "R = np.tile(R[:, np.newaxis, :], (1, K, 1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Store R, Z and X as x and y\n",
    "_x = np.concatenate([Z, R], axis=-1)\n",
    "_y = X\n",
    "\n",
    "# Normalize\n",
    "if _normalize == \"mean\":\n",
    "    mean = np.mean(_x, axis=(0, 1))\n",
    "    std = np.std(_x, axis=(0, 1))\n",
    "    _x = (_x - mean) / (std + np.finfo(float).eps)\n",
    "\n",
    "    _mean = np.mean(_y, axis=(0, 1))\n",
    "    _std = np.std(_y, axis=(0, 1))\n",
    "    _y = (_y - _mean) / (_std + np.finfo(float).eps)\n",
    "elif _normalize == \"max\":\n",
    "    M = np.max(_x, axis=(0, 1))\n",
    "    m = np.min(_x, axis=(0, 1))\n",
    "    _x = (_x - m) / (M - m + np.finfo(float).eps)\n",
    "\n",
    "    _M = np.max(_y, axis=(0, 1))\n",
    "    _m = np.min(_y, axis=(0, 1))\n",
    "    _y = (_y - _m) / (_M - _m + np.finfo(float).eps)\n",
    "elif _normalize is None:\n",
    "    pass\n",
    "else:\n",
    "    raise(\n",
    "        NameError(f'Normalize method \"{_normalize}\" not understood.'))\n",
    "\n",
    "# Convert to float32\n",
    "_x = torch.Tensor(_x)\n",
    "_y = torch.Tensor(_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rescaling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def rescale(_normalize,\n",
    "            y: np.ndarray,\n",
    "            idx_label: int) -> torch.Tensor:\n",
    "    \"\"\"Rescale output from initial normalization.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    y:\n",
    "        Array to resize, of shape (K,).\n",
    "    idx_label:\n",
    "        Index of the output label.\n",
    "    \"\"\"\n",
    "    if _normalize == \"max\":\n",
    "        return y * (_M[idx_label] - _m[idx_label] + np.finfo(float).eps) + _m[idx_label]\n",
    "    elif _normalize == \"mean\":\n",
    "        return y * (_std[idx_label] + np.finfo(float).eps) + _mean[idx_label]\n",
    "    else:\n",
    "        raise(\n",
    "            NameError(f'Normalize method \"{_normalize}\" not understood.'))\n",
    "\n",
    "def __getitem__(idx):\n",
    "    if torch.is_tensor(idx):\n",
    "        idx = idx.tolist()\n",
    "    return (_x[idx], _y[idx])\n",
    "\n",
    "def __len__(_x):\n",
    "    return _x.shape[0]\n",
    "\n",
    "print(_x.size())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([7500, 672, 37])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Converting dataset into a version with a windowed time dimension"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "labels_path: Optional[str] = \"labels.json\"\n",
    "window_size: Optional[int] = 5\n",
    "padding: Optional[int] = 1\n",
    "\n",
    "print(_x.shape)\n",
    "print(_y.shape)\n",
    "    \n",
    "m, K, d_input = _x.shape\n",
    "_, _, d_output = _y.shape\n",
    "\n",
    "step = window_size - 2 * padding\n",
    "n_step = (K - window_size - 1) // step + 1\n",
    "\n",
    "dataset_x = np.empty((m, n_step, window_size, d_input), dtype=np.float32)\n",
    "dataset_y = np.empty((m, n_step, step, d_output), dtype=np.float32)\n",
    "\n",
    "for idx_step, idx in enumerate(range(0, K-window_size, step)):\n",
    "    dataset_x[:, idx_step, :] = _x[:, idx:idx+window_size, :]\n",
    "    dataset_y[:, idx_step, :] = _y[:,idx+padding:idx+window_size-padding, :]\n",
    "\n",
    "_x = dataset_x\n",
    "_y = dataset_y\n",
    "\n",
    "print(_x.size())"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-451d3e2395f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0m_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize CUDA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "summary = torch.cuda.memory_summary(device=None, abbreviated=False).splitlines()\n",
    "print(len(summary))\n",
    "for i in range(0,8):\n",
    "    print(summary[i])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "39\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "print(type(_x))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating a custom dataset class for splitting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x, y, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return (self._x[idx], self._y[idx])\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(type(_x))\n",
    "print(_x.size())\n",
    "ozeInstance = CustomDataset(_x, _y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6849af5b3a9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mozeInstance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "dataset_train, dataset_val, dataset_test = random_split(ozeInstance, (5500, 1000, 1000))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "print(dataset_train.dataset._x.size())\n",
    "print(dataset_train.dataset._y.size())"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-22b4618e3061>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "dataset_train.dataset._y[5, :, :].max(-1)[0].size()\n",
    "print(dataset_train.dataset._x[0:5, :, :].size())\n",
    "print(dataset_train.dataset._x[5, :, :].size())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([5, 672, 37])\n",
      "torch.Size([672, 37])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defining LSTM model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "class pytorch_LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class to define LSTM model using pytorch.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim, num_layers, dropout):\n",
    "        super(pytorch_LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.states = (0, 0)\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, dropout=dropout)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.final = torch.nn.Linear(self.hidden_dim, self.output_dim)\n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        for n, p in m.named_parameters():\n",
    "            if 'weight_ih' in n:\n",
    "                for ih in p.chunk(4,0):\n",
    "                    torch.nn.init.xavier_uniform_(ih)\n",
    "            elif 'weight_hh' in n:\n",
    "                for hh in p.chunk(4,0):\n",
    "                    torch.nn.init.orthogonal_(hh)\n",
    "            elif 'bias_ih' in n:\n",
    "                torch.nn.init.zeros_(p)\n",
    "            elif 'bias_hh' in n:\n",
    "                torch.nn.init.zeros_(p)\n",
    "            elif 'final.weight' in n:\n",
    "                torch.nn.init.xavier_uniform_(p)\n",
    "            elif 'final.bias' in n:\n",
    "                torch.nn.init.zeros_(p)\n",
    "\n",
    "    def initialise_states(self):\n",
    "        \"\"\"\n",
    "        Reset both cell state and hidden state\n",
    "        \"\"\"\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim), torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def reset_hidden(self, states):\n",
    "        \"\"\"\n",
    "        Reset hidden state\n",
    "        \"\"\"\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim), states[1])\n",
    "\n",
    "    def reset_cell(self):\n",
    "        \"\"\"\n",
    "        Reset cell state\n",
    "        \"\"\"\n",
    "        return (states[0], torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input, states):\n",
    "        \"\"\"\n",
    "        Define forward pass through LSTM\n",
    "        \"\"\"\n",
    "        # pass through LSTM layers\n",
    "        lstm_out, states = self.lstm(input.view(len(input), self.batch_size, -1), states)\n",
    "        # pass through linear layer\n",
    "        y_pred = self.final(self.dropout(lstm_out[-1].view(self.batch_size, -1)))\n",
    "        return y_pred.view(-1), states\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Building our model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "class LSTM_pytorch(object):\n",
    "    \n",
    "    def __init__(self, num_layers=2, cells_per_layer=50, dropout=0.5, seed=None, stateful=True, lag=5):\n",
    "        \"\"\"\n",
    "        Initialise and build the model\n",
    "        \"\"\"\n",
    "        self.num_layers = num_layers\n",
    "        self.cells_per_layer = cells_per_layer\n",
    "        self.dropout = dropout\n",
    "        self.seed = seed\n",
    "\n",
    "        self.stateful = stateful\n",
    "        self.lag = lag\n",
    "\n",
    "        if seed != None:\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        \n",
    "    def build(self, subset, debug=False):\n",
    "        \"\"\"\n",
    "        Build model\n",
    "        \"\"\"\n",
    "        self.features = subset.dataset._x.size()[2]\n",
    "\n",
    "        self.sequence = subset.dataset\n",
    "\n",
    "        self.model = pytorch_LSTM(input_dim=self.features, hidden_dim=self.cells_per_layer, batch_size=1, output_dim=self.features, num_layers=self.num_layers, dropout=self.dropout)\n",
    "        if self.features != 1:\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        else:\n",
    "            self.loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        self.model.init_weights(self.model)\n",
    "\n",
    "\n",
    "    def construct_training_index(self, debug=False):\n",
    "        \"\"\"\n",
    "        Construct training index (compatible with model) from sequence of vectors of dimension d,\n",
    "        \"\"\"\n",
    "        n = self.sequence._x.size()[2]\n",
    "        self.index = []\n",
    "        if self.stateful:\n",
    "            # Create groups\n",
    "            self.num_augs = min(self.lag, n - self.lag)\n",
    "            for el in range(self.num_augs):\n",
    "                self.index.append(np.arange(el, n - self.lag, self.lag))\n",
    "        else:\n",
    "            self.num_augs = 1\n",
    "            self.index = np.arange(0, n - self.lag, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, patience=10, max_epoch=1000, acceptable_loss=np.inf, batch_size=1, weight_restarts=False, debug=False):\n",
    "        \"\"\"\n",
    "        Train the model on the constructed training data\n",
    "        \"\"\"\n",
    "        if batch_size != 1:\n",
    "            warnings.warn('batch_size must equal 1, setting to 1.')\n",
    "        ########################################################################\n",
    "        # Weight restarts\n",
    "        ########################################################################\n",
    "        states = self.model.initialise_states()\n",
    "        if weight_restarts:\n",
    "            weight_restarts = 10\n",
    "            store_weights = [0]*weight_restarts\n",
    "            initial_loss = [0]*weight_restarts\n",
    "            for i in range(weight_restarts):\n",
    "                # reset cell state\n",
    "                states = self.model.initialise_states()\n",
    "\n",
    "                y_pred, states = self.model(self.sequence._x[0, self.lag, :], (states[0].detach(), states[1].detach()))\n",
    "\n",
    "                # calculate loss\n",
    "                if self.features == 1:\n",
    "                    self.loss = self.loss_fn(y_pred.view(-1, 1), self.sequence._y[0, self.lag, :])\n",
    "                else:\n",
    "                    target = self.sequence._y[0, self.lag, :] #.max(-1)[1]\n",
    "                    self.loss = self.loss_fn(y_pred.reshape(1,-1), target)\n",
    "\n",
    "                initial_loss[i] = self.loss.data\n",
    "                store_weights[i] = self.model.state_dict()\n",
    "\n",
    "                # Re initialise weights\n",
    "                self.model.init_weights(self.model)\n",
    "            m = np.argmin(initial_loss)\n",
    "            self.model.load_state_dict(store_weights[int(m)])\n",
    "            del store_weights\n",
    "\n",
    "        ########################################################################\n",
    "        # Train\n",
    "        ########################################################################\n",
    "        vec_loss = np.zeros(max_epoch)\n",
    "        min_loss = np.inf\n",
    "        min_loss_ind = np.inf\n",
    "        losses = [0]*self.num_augs\n",
    "        if self.stateful: # no shuffle and reset state manually\n",
    "            for iter in range(max_epoch):\n",
    "                rint = np.random.permutation(self.num_augs) # shuffle groups\n",
    "                for r in rint: # run through groups\n",
    "                    # reset cell state\n",
    "                    states = self.model.initialise_states()\n",
    "\n",
    "                    loss_sum = 0\n",
    "                    for i in self.index[r]: # run through group\n",
    "                        # Forward pass\n",
    "                        print(self.sequence._x[0, i, :].size())\n",
    "                        y_pred, states = self.model(self.sequence._x[0, i, :], (states[0].detach(), states[1].detach()))\n",
    "\n",
    "                        # calculate loss\n",
    "                        if self.features == 1:\n",
    "                            self.loss = self.loss_fn(y_pred.view(-1, 1), self.sequence._y[0, i, :])\n",
    "                        else:\n",
    "                            target = self.sequence._y[0, i, :] #.max(-1)[1]\n",
    "                            self.loss = self.loss_fn(y_pred.reshape(1,-1), target)\n",
    "\n",
    "                        loss_sum += (float(self.loss.data))**2\n",
    "\n",
    "                        # Backward pass\n",
    "                        self.loss.backward(retain_graph=True)\n",
    "\n",
    "                        # Update parameters\n",
    "                        self.optimizer.step()\n",
    "                        # clear gradients\n",
    "                        self.model.zero_grad()\n",
    "\n",
    "                    losses[r] = loss_sum/len(self.index[r])\n",
    "                vec_loss[iter] = np.mean(losses)\n",
    "\n",
    "                if vec_loss[iter] >= min_loss:\n",
    "                    if iter - min_loss_ind >= patience and min_loss<acceptable_loss:\n",
    "                        break\n",
    "                else:\n",
    "                    min_loss = vec_loss[iter]\n",
    "                    old_weights = self.model.state_dict()\n",
    "                    min_loss_ind = iter\n",
    "\n",
    "        else: # shuffle in fit\n",
    "            for iter in range(max_epoch):\n",
    "                loss_sum = 0\n",
    "                for i in np.random.permutation(len(self.index)):\n",
    "                    states = self.model.initialise_states()\n",
    "\n",
    "                    # Forward pass\n",
    "                    y_pred, states = self.model.forward(self.sequence._x[0, i, :], (states[0].detach(), states[1].detach()))\n",
    "\n",
    "                    # calculate loss\n",
    "                    if self.features == 1:\n",
    "                        self.loss = self.loss_fn(y_pred.view(-1, 1), self.sequence._y[0, i, :])\n",
    "                    else:\n",
    "                        target = self.sequence._y[0, i, :].max(-1)[1]\n",
    "                        self.loss = self.loss_fn(y_pred.reshape(1,-1), target)\n",
    "\n",
    "                    loss_sum += (float(self.loss.data))**2\n",
    "\n",
    "                    # Backward pass\n",
    "                    self.loss.backward()\n",
    "\n",
    "                    # Update parameters\n",
    "                    self.optimizer.step()\n",
    "                    # clear gradients\n",
    "                    self.model.zero_grad()\n",
    "\n",
    "                vec_loss[iter] = loss_sum/len(self.index)\n",
    "\n",
    "                if vec_loss[iter] >= min_loss:\n",
    "                    if iter - min_loss_ind >= patience and min_loss < acceptable_loss:\n",
    "                        break\n",
    "                else:\n",
    "                    min_loss = vec_loss[iter]\n",
    "                    old_weights = self.model.state_dict()\n",
    "                    min_loss_ind = iter\n",
    "\n",
    "        self.model.load_state_dict(old_weights)\n",
    "        self.epoch = iter+1\n",
    "        self.loss = vec_loss[0:iter+1]\n",
    "\n",
    "    def forecast(self, k, randomize=False, debug=False):\n",
    "        \"\"\"\n",
    "        Make k step forecast into the future.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        prediction = self.sequence.clone()\n",
    "\n",
    "        # Recursively make k one-step forecasts\n",
    "        for ind in range(len(self.sequence), len(self.sequence) + k):\n",
    "            # Build data to feed into model\n",
    "            if self.stateful:\n",
    "                index = np.arange(ind%self.lag, ind, self.lag)\n",
    "            else:\n",
    "                index = [ind - self.lag]\n",
    "\n",
    "            # Feed through model\n",
    "            states = self.model.initialise_states()\n",
    "            for i in index:\n",
    "                p, states = self.model.forward(prediction[0, i, :], (states[0].detach(), states[1].detach()))\n",
    "\n",
    "            # Convert output\n",
    "            if self.features != 1:\n",
    "                softmax = torch.nn.Softmax(dim=-1)\n",
    "                p = softmax(p).tolist()\n",
    "                p = np.array(p)\n",
    "                p /= p.sum()\n",
    "                if randomize:\n",
    "                    idx = np.random.choice(range(self.features), p=(p.ravel()))\n",
    "                else:\n",
    "                    idx = np.argmax(list(p), axis = 0)\n",
    "\n",
    "                # Add forecast result to appropriate vectors.\n",
    "                pred = torch.zeros([1, 1, self.features])\n",
    "                pred[0, 0, idx] = 1\n",
    "            else:\n",
    "                pred = torch.zeros([1, 1, 1])\n",
    "                pred[0, 0, 0] = p\n",
    "\n",
    "            prediction = torch.cat([prediction, pred], dim=0)\n",
    "\n",
    "        if self.features != 1:\n",
    "            return prediction.view(-1, self.features).tolist()\n",
    "        else:\n",
    "            return prediction.view(-1).detach()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Forcasting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "model = LSTM_pytorch()\n",
    "model.build(dataset_train)\n",
    "model.construct_training_index()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "# Train\n",
    "patience=100\n",
    "max_epoch=100000\n",
    "acceptable_loss=np.inf\n",
    "batch_size=128\n",
    "\n",
    "model.train(patience=patience, max_epoch=max_epoch, acceptable_loss=acceptable_loss, batch_size=batch_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([7500, 37])\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\rohan\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:60: UserWarning: batch_size must equal 1, setting to 1.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "shape '[7500, 7500, -1]' is invalid for input of size 277500",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-6e798ed83483>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macceptable_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0macceptable_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-98-60cf08316a82>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, patience, max_epoch, acceptable_loss, batch_size, weight_restarts, debug)\u001b[0m\n\u001b[0;32m    107\u001b[0m                         \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                         \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                         \u001b[1;31m# calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-2369b441a967>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, states)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \"\"\"\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# pass through LSTM layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;31m# pass through linear layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[7500, 7500, -1]' is invalid for input of size 277500"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}